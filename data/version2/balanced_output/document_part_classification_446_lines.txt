In this method, we first build a SVM to separate positive relationships from negative, neutral and
no-relationship.
We then build a SVM to separate negative from neutral and no- relationship.
Finally we separate neutral from no-relationship.
Such an ordering is chosen on the basis of both manual observations and empirical evaluation.
Here we build a SVM to separate neutral and no-relationships first from positive and negative ones.
We then build two other SVMs to separate between neutral and no-relationship and between positive
and negative relationships, respectively.
The above combination strategy is based on analyses of our dataset, which shows that positive and
negative relationships often exhibit similar characteristics when compared against the other two
types.
For strength analysis we build the SVRs using the entire training set without categorizing the
existing records according to polarity.
We also tried a variation of this approach by building individual SVRs based on polarity but found
that polarity has no effect on the strength analysis.
We evaluate four kernel functions when building a SVM or a SVR, including linear, sigmoid,
polynomial and radial-bias function (RBF).
We explore a variety of kernel combinations for building the multi-stage classifiers for polarity
analysis as reported in the next section.
In this section, we report the main results to demonstrate that the proposed feature space can
effectively predict the polarity and strength of the relationships extracted from biomedical
literature.
These results also indicate that not every feature contributes equally to the two problems under
study.
The annotated corpus described in section 2 is used for our evaluation studies.
(See Tables 4-5 for the corpus distribution according to polarity and strength.) We perform 10-fold
cross validation throughout our evaluation.
Classification accuracy is primarirly used to measure the performance of each SVM, whereas
prediction accuracy is used for each SVR.
We use the SVMLight package by [23] for our experiments.
Table 6 lists the model accuracy for polarity analysis based on various feature combinations and
kernel functions.
To understand this table, let us take an example of the row with feature combination (2+5) for the
One vs.
All method.
(2+5) represents the feature combination of POS-based unigrams with WordNet correction and unary
semantics-based sequential structures.
The column L1 lists the best kernel function used to separate the positive polarity from the rest,
which is the polynomial kernel for the feature set (2+5).
The column L2 gives the best kernel function used to separate negative from (neutral and no-
relationship).
The same level2 kernel is used to separate neutral from no-relationship.
In the case of (2+5), the RBF kernel delivers the best result.  The columns +, -, = and .
list the average accuracy for each of the polarities after 10-fold cross validation using the.
kernel functions under columns L1 and L2.
The overall accuracy calculates the accuracy of a given model over all four polarity classes using
10-fold cross validation.
This is listed under the column OA.
Finally, the column SE indicates the standard error of the overall accuracy.
The highlighted columns represent the best overall accuracy obtained after 10-fold cross validations
including both one vs.
all and two vs.  two methods.
Table 6 Polarity classification results using the 2 SVM schemas and different feature sets.
Column notations: L1-- level 1 of the SVM, L2--level 2 of the SVM, L--linear kernel, P--polynomial
kernel), R--RBF kernel, OA--overall accuracy, + (Positive), - (Negative), = (Neutral), .
(No-relationship).
Feature notations: 1--Penn Treebank based Unigram, 2-- unigrams with WordNet based POS correction,
3--Binary semantics-based features), 4--K-ary semantics-based features, 5--unary semantics based
features, and 6--bigrams.
The top three models are highlighted.  Note that the standard error is that of the overall accuracy.
SVM:  One  vs. All Schema
Feature
Set         Kernel              Average Accuracy            StdErr
L1    L2      +      -      =       !      OA
1         R      R     0.88    0.9   0.87    0.88    0.87   0.0289
1+3       L      P     0.83    0.9   0.73    1       0.84   0.0312
1+4       P      P     0.85    0.9   0.67    1       0.84   0.0303
1+5       L      L     0.78    0.8   0.73    1       0.8    0.0339
1+6       R      R     0.85    0.9   0.73    1       0.85   0.0145
2         R      R     0.87    0.8   0.87    0.88    0.86   0.027
2+3       L      R     0.8     0.9   0.93    1       0.86   0.0245
2+4       P      P     0.87    0.9   0.73    1       0.86   0.014
2+5       P      R     0.88    0.9   0.87    1       0.89   0.0222
2+6       R      R     0.85    0.8   0.87    1       0.86   0.031
2+3+6     R      R     0.8     0.7   0.93    1       0.84   0.0204
2+4+6     P      P     0.9     0.7   0.73    1       0.85   0.0177
2+5+6     L      R     0.83    0.7   0.8     1       0.82   0.0235
SVM:   Two vs. Two  Schema
Feature Set Kernel              Average Accuracy            StdErr
L1    L2      +      -      =       !      OA
1         L      P     0.95    0.9   0.6     1       0.88   0.0189
1+3       R      P     0.98    0.9   0.4     1       0.85   0.0261
1+4       L      P     0.95    0.9   0.73    1       0.91   0.0234
1+5       R      P     0.98    0.9   0.4     1       0.85   0.0108
1+6       R      P     0.92    0.8   0.6     1       0.85   0.014
2         R      R     0.98    0.9   0.53    1       0.88   0.0239
2+3       L      P     0.95    0.8   0.4     1       0.82   0.0144
2+4       L      P     0.95    0.9   0.73    1       0.91   0.0118
2+5       R      P     0.97    0.9   0.47    1       0.86   0.0262
2+6       R      P     0.97    0.7   0.53    1       0.85   0.0189
2+3+6     R      L     0.93    0.9   0.53    1       0.85   0.0203
2+4+6     L      L     0.9     0.9   0.73    1       0.88   0.017
2+5+6     L      P     0.97    0.6   0.4     1       0.81   0.0139
From Table 6, one can observe that the positive polarity constantly has high accuracy for both
methods as compared to the other polarities.
One main reason we believe is that the annotated corpus has a large number of positive examples as
shown in Table 4.
We also observe that the “no-relationship” has a constantly high accuracy and is not influenced by
other feature combinations.
The reason behind this is that the unigrams found in the relationships that have “no relationship”
polarity often contain unique negation terms such as “no” and ‘not”.
Therefore unigrams alone are often sufficient.
The accuracy of neutral relationships is low because neutral and positive relationships tend to
contain.
