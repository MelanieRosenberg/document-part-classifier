<TEXT>
identical unigrams and exhibit similar semantics-based sequential structures. Hence SVM is not able to differentiate between positive and neutral relationships. To give an example the below sentence has features which are the same as a positive relationship but is labeled as neutral due to lack of contextual information. {\chemical P-ASA} {\relationship 2= increased} intracellular levels of {\chemical reactive oxygen species}. Finally we can also observe that the (1 vs. All) SVM schema behaves differently than the (2 vs. 2). The highest overall accuracy of (1 vs. all) is 0.89, generated by a feature combination of unigrams and unary semantics-based sequential features. We cannot however find any particular combination of kernels that constantly produce a high accuracy. In contrast, in the (2 vs. 2) schema, the linear and polynomial kernel combination tends to produce good results. In addition, (2 vs. 2) in general outperforms (1 vs. all) regardless of the feature set under use. We hence focus on the (2 vs. 2) scheme in the following discussion.
</TEXT>
<TEXT>
For the (2 vs. 2) SVM schema, one can observe from Table 6 that the highest overall accuracy is 0.91 and generated by a feature combination of POS-based unigrams and K-array semantics-based sequential features, regardless of whether WordNet is used to correct the POS of a unigram or not. In terms of the kernel functions, a linear kernel is used at the first level of the SVM model and a polynomial kernel at the second level. In addition, we notice that the inclusion of bigrams (the last four rows in Table 6) does not improve accuracy but rather reduce the accuracy. This agrees with results reported in previous studies. Tables 7 and 8 present the confusion matrices resulted from the two best feature sets using the (2 vs. 2) SVM schema. These two tables elaborate the two optimal feature sets as highlighted in Table 6. Results are obtained by using a linear and a polynomial kernel at the two levels respectively and averaged over 10-fold cross validation. Notice that in certain cases, the model cannot ditinguish between positive and neutral polarities due to the reasons discussed above; while negative and no-relationship can often be correctly identified.
</TEXT>
<TABLE>
Actual
Predicted     Positive   Negative    Neutral     No-Rel.
  Positive      39          1           4           0
 Negative        0          9           0           0
  Neutral        2          0           11          0
  No-Rel         0          0           0           8
</TABLE>
<CAPTION>
Table 8. Confusion matrix from using another optimal set of features: (Penn Treebank with WordNet correction)-based Unigrams and K-ary semantics-based sequential structures. This elaborates the third highlighted row in Table 6.
</CAPTION>
<TABLE>
Actual
Predicted     Positive   Negative    Neutral     No-Rel.
  Positive      40          1           3           0
 Negative        0          9           0           0
  Neutral        3          0           12          0
  No-Rel         0          0           0           8
</TABLE>
<TEXT>
decreases the accuracy levels as shown in Table 9. The main reason is that it might just add redundant or even incorrect information for creating semantic structures due to limitations in the current co-reference identification module. Table 9 shows that the overall accuracy drops when including co-references, regardless of the feature set being used to construct the (2 vs. 2) SVM classifier .
</TEXT>
<CAPTION>
Table 9. The impact of co-references on the overall accuracy using the (2 vs. 2) SVM schema.
</CAPTION>
<TABLE>
POSbased    POSbased    POS-based  POS (Comection). POS (Correction)- POS (Conrection)-
                Unigram     Unigram     Unigram    —basedUnigran  based   Unigram  based  Unigram
Features        HSS         -+BTSS.     KS.        HUSS            +BISS            +Kss
Coxeference     08          079         087        Ot               08              0.99
No  Co-teference 0.85       0.85        091        0.86             0.82            091
</TABLE>
<TEXT>
Another seemingly counter-intuitive result is that constructing unigram features from neighborhood of a relationship-depicting phrase (RDP) actually degrades the performance as against from the RDP only. The main reason being that more than 50% of the unigrams formed using the neighborhood have a term and document frequency of 1 or 2. This in turn has introduced irrelevant features to the model. Table 10 shows that the overall accuracy reduces to 0.85 from 0.88 for the Two vs. Two Method when using the entire RDP neighborhood to form unigrams.
</TEXT>
<CAPTION>
Table 10. The impact of unigrams constructed from two different neighborhoods on the overall accuracy.
</CAPTION>
<TABLE>
Features               Relationship based Unigram  E-R-ER-E/ER  boundary  based Unigram
POS  without comection  0.88                       os
POS  with conection     0.88                       082
</TABLE>
<TEXT>
Finally, we have also compared the effect of the no-relationship polarity by comparing the overall accuracy of (i) combining no- relationship and neutral polarities into one category; and (ii) separating them into two classes. We observe that the overall accuracy of the former ranges between 0.71~0.81 as compared to 0.8~0.91 in the latter when they are separated. This demonstrates the necessity of introducing the “no-relationship” as its own class.
</TEXT>
<SECTION_HEADER>
5.2 Strength Analysis Results
</SECTION_HEADER>
<TEXT>
We have also built various SVR models using different feature combinations fro strength analysis. The average accuracy from 10-fold cross validation is shown in Table 11. Note that all the results are generated using a linear kernel based SVR. From Table 11 we observe that all the feature combinations deliver approximately similar high-quality results. In other words, the addition of bigrams and semantics-based structural features does not improve the overall performance. This indicates that using unigrams alone is sufficient for the strength prediction task. The main reason behind this phenomenon is that unlike polarity of a relationship, the strength of a relationship is often directly associated with the specific words used in the relationship- depicting phrase. For instance, the sentence “Soy consumption significantly reduces the risk of cancer.” has a strong strength. The word “significantly” carries the most weight for the SVR model to make the correct prediction. This also explains why the addition of other semantics based features does not help in general. Another observation we have made is that the linear kernel constantly outperforms the other kernels. Finally, we notice that the medium strength achieves the highest accuracy as against
</TEXT>
<PAGE_BREAK>
