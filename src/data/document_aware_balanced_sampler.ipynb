{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad093189-a461-48a1-8453-d269e5aca679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 16:02:31,313 - INFO - Read 656 documents\n",
      "2025-03-22 16:02:31,315 - INFO - Class distribution: {'FORM': 7437, 'TEXT': 7673, 'TABLE': 4524}\n",
      "2025-03-22 16:02:31,326 - INFO - Created balanced dataset with 13572 total examples\n",
      "2025-03-22 16:02:31,326 - INFO - Final class distribution: {'FORM': 4524, 'TEXT': 4524, 'TABLE': 4524}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_balanced_dataset(input_dir, output_dir, target_count=4524):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset by:\n",
    "    1. Keeping all TABLE lines\n",
    "    2. Randomly sampling TEXT and FORM lines to match the number of TABLE lines\n",
    "    3. Preserving document ordering to enable proper train/val/test splits later\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory with processed lines and labels files\n",
    "        output_dir: Directory to save the balanced dataset\n",
    "        target_count: Number of examples to sample for each class (default: 4524 for TABLE)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Read all files and organize by document\n",
    "    documents = {}\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if not filename.endswith('_lines.txt'):\n",
    "            continue\n",
    "            \n",
    "        doc_id = filename.replace('_lines.txt', '')\n",
    "        lines_file = os.path.join(input_dir, filename)\n",
    "        labels_file = os.path.join(input_dir, filename.replace('_lines.txt', '_labels.txt'))\n",
    "        \n",
    "        if not os.path.exists(labels_file):\n",
    "            logger.warning(f\"Labels file not found for {lines_file}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Read files\n",
    "        with open(lines_file, 'r', encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "            labels = [label.strip() for label in f.readlines()]\n",
    "        \n",
    "        documents[doc_id] = {\n",
    "            'lines': lines,\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Read {len(documents)} documents\")\n",
    "    \n",
    "    # Step 2: Create class-specific indices per document\n",
    "    class_indices = defaultdict(list)\n",
    "    \n",
    "    for doc_id, doc in documents.items():\n",
    "        for i, label in enumerate(doc['labels']):\n",
    "            # Store as tuple: (document_id, line_index)\n",
    "            class_indices[label].append((doc_id, i))\n",
    "    \n",
    "    # Count instances per class\n",
    "    class_counts = {label: len(indices) for label, indices in class_indices.items()}\n",
    "    logger.info(f\"Class distribution: {class_counts}\")\n",
    "    \n",
    "    # Step 3: Sample from TEXT and FORM while keeping all TABLE\n",
    "    sampled_indices = {}\n",
    "    \n",
    "    # Keep all TABLE instances\n",
    "    sampled_indices['TABLE'] = class_indices['TABLE']\n",
    "    \n",
    "    # Sample from TEXT and FORM to match target count\n",
    "    for label in ['TEXT', 'FORM']:\n",
    "        if len(class_indices[label]) <= target_count:\n",
    "            # Keep all if less than target\n",
    "            sampled_indices[label] = class_indices[label]\n",
    "        else:\n",
    "            # Random sample without replacement\n",
    "            sampled_indices[label] = random.sample(class_indices[label], target_count)\n",
    "    \n",
    "    # Step 4: Collect all sampled data while preserving document order\n",
    "    all_indices = []\n",
    "    for label in ['TEXT', 'FORM', 'TABLE']:\n",
    "        all_indices.extend(sampled_indices[label])\n",
    "    \n",
    "    # Sort by document ID and then by line index to preserve document ordering\n",
    "    all_indices.sort()\n",
    "    \n",
    "    # Create final lists\n",
    "    balanced_lines = []\n",
    "    balanced_labels = []\n",
    "    doc_boundaries = {}  # Track where each document starts/ends\n",
    "    \n",
    "    current_doc = None\n",
    "    for doc_id, line_idx in all_indices:\n",
    "        if current_doc != doc_id:\n",
    "            if current_doc is not None:\n",
    "                doc_boundaries[current_doc]['end'] = len(balanced_lines) - 1\n",
    "            \n",
    "            current_doc = doc_id\n",
    "            doc_boundaries[current_doc] = {'start': len(balanced_lines)}\n",
    "        \n",
    "        balanced_lines.append(documents[doc_id]['lines'][line_idx])\n",
    "        balanced_labels.append(documents[doc_id]['labels'][line_idx])\n",
    "    \n",
    "    # Add the final document end\n",
    "    if current_doc is not None:\n",
    "        doc_boundaries[current_doc]['end'] = len(balanced_lines) - 1\n",
    "    \n",
    "    # Step 5: Save the balanced dataset\n",
    "    with open(os.path.join(output_dir, \"all_lines.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for line in balanced_lines:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"all_labels.txt\"), 'w', encoding='utf-8') as f:\n",
    "        for label in balanced_labels:\n",
    "            f.write(f\"{label}\\n\")\n",
    "    \n",
    "    # Save document boundaries for potential document-aware train/val/test split\n",
    "    with open(os.path.join(output_dir, \"document_boundaries.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(doc_boundaries, f, indent=2)\n",
    "    \n",
    "    # Count final class distribution\n",
    "    final_counts = defaultdict(int)\n",
    "    for label in balanced_labels:\n",
    "        final_counts[label] += 1\n",
    "    \n",
    "    logger.info(f\"Created balanced dataset with {len(balanced_lines)} total examples\")\n",
    "    logger.info(f\"Final class distribution: {dict(final_counts)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_dir = \"balanced_output\"\n",
    "    output_dir = \"balanced_sampled_by_document\"\n",
    "    target_count = 4524  # Match the number of TABLE instances\n",
    "    \n",
    "    create_balanced_dataset(input_dir, output_dir, target_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbeaed-d213-4e6e-8add-e4ffdee783c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
